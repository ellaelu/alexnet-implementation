{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import time\n",
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load tinyimage net dataset and return \n",
    "\n",
    "Parameters:\n",
    "----------\n",
    "data_folder: directory to tiny imagenet\n",
    "size: the pixel size for each image\n",
    "batch_size: number of images per batch\n",
    "num_works: set # of works simultaneously inputting data\n",
    "----------\n",
    "\n",
    "Returns:\n",
    "----------\n",
    "data_loader: returns pytorch object of the respective three datasets, 'train', 'val', and 'test'\n",
    "dataset_sizes: returns dict of sizes\n",
    "classes: returns list of all used in label IDs\n",
    "----------\n",
    "\n",
    "\"\"\"\n",
    "def load_data(data_folder = '', size = 150, batch_size = 8, num_workers = 8):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    imagenets = {\n",
    "        'train': ImageFolder(data_folder + '/train', transform=data_transforms['train']),\n",
    "        'val': ImageFolder(data_folder + '/val', transform=data_transforms['val']),\n",
    "        'test': ImageFolder(data_folder + '/test', transform=data_transforms['test'])\n",
    "    }\n",
    "\n",
    "    data_loader = {\n",
    "        x: torch.utils.data.DataLoader(imagenets[x],\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                       num_workers=num_workers) for x in ['train', 'val', 'test']\n",
    "    }\n",
    "    dataset_sizes = {x: len(imagenets[x]) for x in ['train', 'val', 'test']}\n",
    "    print('Datasets Loaded')\n",
    "    classes = imagenets['train'].classes\n",
    "    \n",
    "    return data_loader, dataset_sizes, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load label IDs and match them with their description\n",
    "\n",
    "Parameters:\n",
    "----------\n",
    "label_folder: directory to words.txt within the tiny imagenet directory\n",
    "----------\n",
    "\n",
    "Returns:\n",
    "----------\n",
    "labels_list: returns a list of just the descriptions of all IDs\n",
    "labels_dict: returns dict of the list above, but matched with its ID\n",
    "----------\n",
    "\n",
    "\"\"\"\n",
    "def load_labels(label_folder = ''):\n",
    "    f = open(label_folder, \"r\")\n",
    "    labels_dict = {}\n",
    "    labels_list = []\n",
    "    for line in f:\n",
    "        split = line.split(maxsplit=1)\n",
    "        split[1] = split[1][:-1]\n",
    "        label_id, label = split[0], split[1]\n",
    "        labels_dict[label_id] = label.split(',')[0]\n",
    "        labels_list.append(split[1])\n",
    "    print(\"Labels Loaded\")\n",
    "    return labels_list, labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get label description based on index. Index number represents where it lies in 'classes' array. \n",
    "Using a dict we can return a description rather than an ID.\n",
    "\n",
    "Parameters:\n",
    "----------\n",
    "x: index in classes array, to be transformed to description\n",
    "----------\n",
    "\n",
    "Returns:\n",
    "----------\n",
    "label: returns a string of more than 1 word replacing the ID\n",
    "----------\n",
    "\n",
    "\"\"\"\n",
    "def get_label(x):\n",
    "    label = labels_dict[classes[x]]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Denormalize our images due to our preprocessing early, and convert them to numpy arrays to be viewed using matplotlib.\n",
    "\n",
    "Parameters:\n",
    "----------\n",
    "img: tensor object representing the image\n",
    "title: caption/title of that specific image\n",
    "----------\n",
    "\"\"\"\n",
    "def imshow(img, title=None):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std*img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) #Pause is necessary to display images correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets Loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x22d388e14e0>,\n",
       " 'val': <torch.utils.data.dataloader.DataLoader at 0x22d388e13c8>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x22d3f00d438>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data,sizes,classes = load_data('tinyimage10')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Loaded\n",
      "['physical entity', 'abstraction, abstract entity', 'thing']\n"
     ]
    }
   ],
   "source": [
    "labels_list, labels_dict = load_labels(\"tinyimage10\\\\words.txt\")\n",
    "print(labels_list[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a batch of training data\n",
    "inputs, labels_id = next(iter(data['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs[0:4], nrow = 4)\n",
    "\n",
    "imshow(out, title=[get_label(x) for x in labels_id[0:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reproduced AlexNet model.\n",
    "\"\"\"\n",
    "class AlexNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=200):\n",
    "        \n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "           nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "           nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "           nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "           nn.ReLU(inplace=True),\n",
    "        \n",
    "           nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "           nn.ReLU(inplace=True),\n",
    "            \n",
    "           nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "           nn.Dropout(),\n",
    "           nn.Linear(256*6*6, 4096),\n",
    "           nn.ReLU(inplace=True),\n",
    "           \n",
    "           nn.Dropout(),\n",
    "           nn.Linear(4096, 4096),\n",
    "           nn.ReLU(inplace=True),\n",
    "         \n",
    "           nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.fc(x)   \n",
    "        return x\n",
    "    \n",
    "model = AlexNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train reproduced AlexNet model based on inputs and parameters. Returns the model and time taken per run. \n",
    "Adds metric values to tensorboard Writer file to be viewed.\n",
    "\n",
    "Parameters:\n",
    "----------\n",
    "model: model to use for training\n",
    "data_loader: pytorch object of the respective three datasets, 'train', 'val', and 'test'\n",
    "loss_func: loss function, Cross Entropy Loss used in this case\n",
    "scheduler: scheduler for adjusting learning rate\n",
    "optimizer: optimizer function (only SGD available at the moment)\n",
    "num_epochs: number of epochs per run\n",
    "momentum: used to calculate weight decay\n",
    "step_size: ...\n",
    "gamma: gamma value\n",
    "print_freq: console printing frequency\n",
    "----------\n",
    "\n",
    "Returns:\n",
    "----------\n",
    "model: returns the model back\n",
    "t: returns time taken to run function in minutes\n",
    "----------\n",
    "\n",
    "\"\"\"\n",
    "def train(model, data_loader, loss_func, scheduler, optimizer, num_epochs=20, \n",
    "          lr=0.005, momentum = 0.9, step_size=30, gamma=0.1,print_freq=200):\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    train_batch_loss = []\n",
    "    train_epoch_loss = []\n",
    "    val_epoch_loss = []\n",
    "    progress = []\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-'*15)\n",
    "        \n",
    "        # You perform validation test after every epoch\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0 \n",
    "                \n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # zero accumulated gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # During train phase we want to remember history for grads\n",
    "                # and during val we do not want history of grads\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_func(outputs, labels)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    if i%print_freq == 0:\n",
    "                        train_batch_loss.append(loss.item())\n",
    "                        print('Epoch {}: {}/{} step in progress'.format(epoch+1, i, len(data_loader['train'])))\n",
    "                        \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_loader[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'val':\n",
    "                val_epoch_loss.append((epoch_loss, epoch_acc))\n",
    "                writer.add_scalar('val acc',\n",
    "                epoch_acc,\n",
    "                epoch)\n",
    "                writer.add_scalar('val loss',\n",
    "                epoch_loss,\n",
    "                epoch)\n",
    "                scheduler.step(loss.item())\n",
    "            else:\n",
    "                train_epoch_loss.append((epoch_loss, epoch_acc))\n",
    "                writer.add_scalar('train acc',\n",
    "                epoch_acc,\n",
    "                epoch)\n",
    "                writer.add_scalar('train loss',\n",
    "                epoch_loss,\n",
    "                epoch)\n",
    "                scheduler.step(loss.item())\n",
    "                scheduler.step(loss.item())\n",
    "                \n",
    "            progress.append([epoch_loss, epoch_acc.item()])\n",
    "                \n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    t = np.float(time_elapsed/60)\n",
    "    \n",
    "    return model, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate accuracy of model using an input to test on. \n",
    "Available option to show accuracy of each label, rather than total accuracy.\n",
    "\n",
    "Parameters:\n",
    "----------\n",
    "data: pytorch object dataset (ex: data['train'] or data['test'])\n",
    "labels_acc: True or False, show labels accuracy instead of total accuracy\n",
    "----------\n",
    "\n",
    "Returns:\n",
    "----------\n",
    "acc: accuracy (correct / total)\n",
    "----------\n",
    "\"\"\"\n",
    "def calc_acc(data, labels_acc = False):\n",
    "    if(labels_acc == False):\n",
    "        # Get test accuracy.\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(data):\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (preds == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the network on %2d images: %d %%' % (len(preds),\n",
    "            100 * correct / total))\n",
    "        acc = correct / total\n",
    "        return acc\n",
    "    \n",
    "    if(labels_acc == True):\n",
    "        # Get test accuracy for each class.\n",
    "        class_correct = list(0. for i in range(10))\n",
    "        class_total = list(0. for i in range(10))\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(data):\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                c = (predicted == labels).squeeze()\n",
    "                for i in range(10):\n",
    "                    label = labels[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "            for i in range(10):\n",
    "                print('Accuracy of %5s : %2d %%' % (\n",
    "                    labels_dict[classes[i]], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "# We train everything on GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size' : [12],\n",
    "    'learning_rate' : [ 0.0001, 0.001, 0.005],\n",
    "    'momentum' : [0.9, 0.5],\n",
    "    'gamma' : [0.1, 0.01],\n",
    "    'optimizer' : ['sgd'],\n",
    "    'epoch' : 35\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12/12\n",
      "Datasets Loaded\n",
      "Labels Loaded\n",
      "runs/AlexNet_experiment_0211\n",
      "Epoch 1/35\n",
      "---------------\n",
      "Epoch 1: 0/334 step in progress\n",
      "Epoch 1: 200/334 step in progress\n",
      "train Loss: 2.8904 Acc: 0.0990\n",
      "Epoch 1: 0/334 step in progress\n",
      "val Loss: 2.4170 Acc: 0.1000\n",
      "\n",
      "Epoch 2/35\n",
      "---------------\n",
      "Epoch 2: 0/334 step in progress\n",
      "Epoch 2: 200/334 step in progress\n",
      "train Loss: 2.3780 Acc: 0.1025\n",
      "Epoch 2: 0/334 step in progress\n",
      "val Loss: 2.4286 Acc: 0.1000\n",
      "\n",
      "Epoch 3/35\n",
      "---------------\n",
      "Epoch 3: 0/334 step in progress\n",
      "Epoch 3: 200/334 step in progress\n",
      "train Loss: 2.3629 Acc: 0.1028\n",
      "Epoch 3: 0/334 step in progress\n",
      "val Loss: 2.3948 Acc: 0.1000\n",
      "\n",
      "Epoch 4/35\n",
      "---------------\n",
      "Epoch 4: 0/334 step in progress\n",
      "Epoch 4: 200/334 step in progress\n",
      "train Loss: 2.3460 Acc: 0.1038\n",
      "Epoch 4: 0/334 step in progress\n",
      "val Loss: 2.3137 Acc: 0.1120\n",
      "\n",
      "Epoch 5/35\n",
      "---------------\n",
      "Epoch 5: 0/334 step in progress\n",
      "Epoch 5: 200/334 step in progress\n",
      "train Loss: 2.2992 Acc: 0.1265\n",
      "Epoch 5: 0/334 step in progress\n",
      "val Loss: 2.2725 Acc: 0.1380\n",
      "\n",
      "Epoch 6/35\n",
      "---------------\n",
      "Epoch 6: 0/334 step in progress\n",
      "Epoch 6: 200/334 step in progress\n",
      "train Loss: 2.2190 Acc: 0.1530\n",
      "Epoch 6: 0/334 step in progress\n",
      "val Loss: 2.1822 Acc: 0.1780\n",
      "\n",
      "Epoch 7/35\n",
      "---------------\n",
      "Epoch 7: 0/334 step in progress\n",
      "Epoch 7: 200/334 step in progress\n",
      "train Loss: 2.1108 Acc: 0.2013\n",
      "Epoch 7: 0/334 step in progress\n",
      "val Loss: 2.1473 Acc: 0.2160\n",
      "\n",
      "Epoch 8/35\n",
      "---------------\n",
      "Epoch 8: 0/334 step in progress\n",
      "Epoch 8: 200/334 step in progress\n",
      "train Loss: 2.0371 Acc: 0.2343\n",
      "Epoch 8: 0/334 step in progress\n",
      "val Loss: 2.0736 Acc: 0.2160\n",
      "\n",
      "Epoch 9/35\n",
      "---------------\n",
      "Epoch 9: 0/334 step in progress\n",
      "Epoch 9: 200/334 step in progress\n",
      "train Loss: 1.9588 Acc: 0.2660\n",
      "Epoch 9: 0/334 step in progress\n",
      "val Loss: 2.0213 Acc: 0.2740\n",
      "\n",
      "Epoch 10/35\n",
      "---------------\n",
      "Epoch 10: 0/334 step in progress\n",
      "Epoch 10: 200/334 step in progress\n",
      "train Loss: 1.8991 Acc: 0.2913\n",
      "Epoch 10: 0/334 step in progress\n",
      "val Loss: 1.9796 Acc: 0.2660\n",
      "\n",
      "Epoch 11/35\n",
      "---------------\n",
      "Epoch 11: 0/334 step in progress\n",
      "Epoch 11: 200/334 step in progress\n",
      "train Loss: 1.8461 Acc: 0.3085\n",
      "Epoch 11: 0/334 step in progress\n",
      "val Loss: 1.8586 Acc: 0.2960\n",
      "\n",
      "Epoch 12/35\n",
      "---------------\n",
      "Epoch 12: 0/334 step in progress\n",
      "Epoch 12: 200/334 step in progress\n",
      "train Loss: 1.8102 Acc: 0.3200\n",
      "Epoch 12: 0/334 step in progress\n",
      "val Loss: 1.8995 Acc: 0.3360\n",
      "\n",
      "Epoch 13/35\n",
      "---------------\n",
      "Epoch 13: 0/334 step in progress\n",
      "Epoch 13: 200/334 step in progress\n",
      "train Loss: 1.7734 Acc: 0.3445\n",
      "Epoch 13: 0/334 step in progress\n",
      "val Loss: 1.9479 Acc: 0.2500\n",
      "\n",
      "Epoch 14/35\n",
      "---------------\n",
      "Epoch 14: 0/334 step in progress\n",
      "Epoch 14: 200/334 step in progress\n",
      "train Loss: 1.7391 Acc: 0.3548\n",
      "Epoch 14: 0/334 step in progress\n",
      "val Loss: 1.8121 Acc: 0.3860\n",
      "\n",
      "Epoch 15/35\n",
      "---------------\n",
      "Epoch 15: 0/334 step in progress\n",
      "Epoch 15: 200/334 step in progress\n",
      "train Loss: 1.7163 Acc: 0.3673\n",
      "Epoch 15: 0/334 step in progress\n",
      "val Loss: 1.8345 Acc: 0.3540\n",
      "\n",
      "Epoch 16/35\n",
      "---------------\n",
      "Epoch 16: 0/334 step in progress\n",
      "Epoch 16: 200/334 step in progress\n",
      "train Loss: 1.6658 Acc: 0.3970\n",
      "Epoch 16: 0/334 step in progress\n",
      "val Loss: 1.7268 Acc: 0.3780\n",
      "\n",
      "Epoch 17/35\n",
      "---------------\n",
      "Epoch 17: 0/334 step in progress\n",
      "Epoch 17: 200/334 step in progress\n",
      "train Loss: 1.6063 Acc: 0.4165\n",
      "Epoch 17: 0/334 step in progress\n",
      "val Loss: 1.7155 Acc: 0.4200\n",
      "\n",
      "Epoch 18/35\n",
      "---------------\n",
      "Epoch 18: 0/334 step in progress\n",
      "Epoch 18: 200/334 step in progress\n",
      "train Loss: 1.5620 Acc: 0.4430\n",
      "Epoch 18: 0/334 step in progress\n",
      "val Loss: 1.7311 Acc: 0.4200\n",
      "\n",
      "Epoch 19/35\n",
      "---------------\n",
      "Epoch 19: 0/334 step in progress\n",
      "Epoch 19: 200/334 step in progress\n",
      "train Loss: 1.5298 Acc: 0.4428\n",
      "Epoch 19: 0/334 step in progress\n",
      "val Loss: 1.9511 Acc: 0.3540\n",
      "\n",
      "Epoch 20/35\n",
      "---------------\n",
      "Epoch 20: 0/334 step in progress\n",
      "Epoch 20: 200/334 step in progress\n",
      "train Loss: 1.4763 Acc: 0.4875\n",
      "Epoch 20: 0/334 step in progress\n",
      "val Loss: 1.6090 Acc: 0.4560\n",
      "\n",
      "Epoch 21/35\n",
      "---------------\n",
      "Epoch 21: 0/334 step in progress\n",
      "Epoch 21: 200/334 step in progress\n",
      "train Loss: 1.4349 Acc: 0.5008\n",
      "Epoch 21: 0/334 step in progress\n",
      "val Loss: 1.5678 Acc: 0.4760\n",
      "\n",
      "Epoch 22/35\n",
      "---------------\n",
      "Epoch 22: 0/334 step in progress\n",
      "Epoch 22: 200/334 step in progress\n",
      "train Loss: 1.3795 Acc: 0.5145\n",
      "Epoch 22: 0/334 step in progress\n",
      "val Loss: 1.6211 Acc: 0.4600\n",
      "\n",
      "Epoch 23/35\n",
      "---------------\n",
      "Epoch 23: 0/334 step in progress\n",
      "Epoch 23: 200/334 step in progress\n",
      "train Loss: 1.3367 Acc: 0.5295\n",
      "Epoch 23: 0/334 step in progress\n",
      "val Loss: 1.6425 Acc: 0.4400\n",
      "\n",
      "Epoch 24/35\n",
      "---------------\n",
      "Epoch 24: 0/334 step in progress\n",
      "Epoch 24: 200/334 step in progress\n",
      "train Loss: 1.3080 Acc: 0.5408\n",
      "Epoch 24: 0/334 step in progress\n",
      "val Loss: 1.6360 Acc: 0.4400\n",
      "\n",
      "Epoch 25/35\n",
      "---------------\n",
      "Epoch 25: 0/334 step in progress\n",
      "Epoch 25: 200/334 step in progress\n",
      "train Loss: 1.2433 Acc: 0.5690\n",
      "Epoch 25: 0/334 step in progress\n",
      "val Loss: 1.6000 Acc: 0.4860\n",
      "\n",
      "Epoch 26/35\n",
      "---------------\n",
      "Epoch 26: 0/334 step in progress\n",
      "Epoch 26: 200/334 step in progress\n",
      "train Loss: 1.1980 Acc: 0.5887\n",
      "Epoch 26: 0/334 step in progress\n",
      "val Loss: 1.8068 Acc: 0.4080\n",
      "\n",
      "Epoch 27/35\n",
      "---------------\n",
      "Epoch 27: 0/334 step in progress\n",
      "Epoch 27: 200/334 step in progress\n",
      "train Loss: 1.1600 Acc: 0.5973\n",
      "Epoch 27: 0/334 step in progress\n",
      "val Loss: 1.5063 Acc: 0.4900\n",
      "\n",
      "Epoch 28/35\n",
      "---------------\n",
      "Epoch 28: 0/334 step in progress\n",
      "Epoch 28: 200/334 step in progress\n",
      "train Loss: 1.1276 Acc: 0.6070\n",
      "Epoch 28: 0/334 step in progress\n",
      "val Loss: 1.4823 Acc: 0.5220\n",
      "\n",
      "Epoch 29/35\n",
      "---------------\n",
      "Epoch 29: 0/334 step in progress\n",
      "Epoch 29: 200/334 step in progress\n",
      "train Loss: 1.0814 Acc: 0.6160\n",
      "Epoch 29: 0/334 step in progress\n",
      "val Loss: 1.6063 Acc: 0.5140\n",
      "\n",
      "Epoch 30/35\n",
      "---------------\n",
      "Epoch 30: 0/334 step in progress\n",
      "Epoch 30: 200/334 step in progress\n",
      "train Loss: 1.0452 Acc: 0.6255\n",
      "Epoch 30: 0/334 step in progress\n",
      "val Loss: 1.8456 Acc: 0.4660\n",
      "\n",
      "Epoch 31/35\n",
      "---------------\n",
      "Epoch 31: 0/334 step in progress\n",
      "Epoch 31: 200/334 step in progress\n",
      "train Loss: 1.0040 Acc: 0.6472\n",
      "Epoch 31: 0/334 step in progress\n",
      "val Loss: 1.6262 Acc: 0.4740\n",
      "\n",
      "Epoch 32/35\n",
      "---------------\n",
      "Epoch 32: 0/334 step in progress\n",
      "Epoch 32: 200/334 step in progress\n",
      "train Loss: 0.9819 Acc: 0.6512\n",
      "Epoch 32: 0/334 step in progress\n",
      "val Loss: 1.6455 Acc: 0.4940\n",
      "\n",
      "Epoch 33/35\n",
      "---------------\n",
      "Epoch 33: 0/334 step in progress\n",
      "Epoch 33: 200/334 step in progress\n",
      "train Loss: 0.9244 Acc: 0.6777\n",
      "Epoch 33: 0/334 step in progress\n",
      "val Loss: 1.8018 Acc: 0.4700\n",
      "\n",
      "Epoch 34/35\n",
      "---------------\n",
      "Epoch 34: 0/334 step in progress\n",
      "Epoch 34: 200/334 step in progress\n",
      "train Loss: 0.8815 Acc: 0.6943\n",
      "Epoch 34: 0/334 step in progress\n",
      "val Loss: 1.7190 Acc: 0.5220\n",
      "\n",
      "Epoch 35/35\n",
      "---------------\n",
      "Epoch 35: 0/334 step in progress\n",
      "Epoch 35: 200/334 step in progress\n",
      "train Loss: 0.8489 Acc: 0.7047\n",
      "Epoch 35: 0/334 step in progress\n",
      "val Loss: 1.6181 Acc: 0.5520\n",
      "\n",
      "Training complete in 10m 36s\n",
      "Accuracy of the network on  4 images: 73 %\n",
      "Accuracy of the network on  8 images: 55 %\n",
      "Accuracy of the network on  8 images: 53 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>momentum</th>\n",
       "      <th>gamma</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>time(m)</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>runs/AlexNet_experiment_0000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>11.339211</td>\n",
       "      <td>0.16550</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>runs/AlexNet_experiment_0001</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.766934</td>\n",
       "      <td>0.16000</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>runs/AlexNet_experiment_0010</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.802704</td>\n",
       "      <td>0.10650</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>runs/AlexNet_experiment_0011</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.881917</td>\n",
       "      <td>0.10325</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>runs/AlexNet_experiment_0100</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.824979</td>\n",
       "      <td>0.77850</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>runs/AlexNet_experiment_0101</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.823393</td>\n",
       "      <td>0.72800</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>runs/AlexNet_experiment_0110</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>11.056019</td>\n",
       "      <td>0.31725</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>runs/AlexNet_experiment_0111</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>11.018509</td>\n",
       "      <td>0.32650</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>runs/AlexNet_experiment_0200</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>11.177425</td>\n",
       "      <td>0.68500</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>runs/AlexNet_experiment_0201</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.675909</td>\n",
       "      <td>0.70275</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>runs/AlexNet_experiment_0210</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.631684</td>\n",
       "      <td>0.72800</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>runs/AlexNet_experiment_0211</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.594885</td>\n",
       "      <td>0.73750</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file  batch_size  learning_rate  momentum  gamma  \\\n",
       "0   runs/AlexNet_experiment_0000          12         0.0001       0.9   0.10   \n",
       "1   runs/AlexNet_experiment_0001          12         0.0001       0.9   0.01   \n",
       "2   runs/AlexNet_experiment_0010          12         0.0001       0.5   0.10   \n",
       "3   runs/AlexNet_experiment_0011          12         0.0001       0.5   0.01   \n",
       "4   runs/AlexNet_experiment_0100          12         0.0010       0.9   0.10   \n",
       "5   runs/AlexNet_experiment_0101          12         0.0010       0.9   0.01   \n",
       "6   runs/AlexNet_experiment_0110          12         0.0010       0.5   0.10   \n",
       "7   runs/AlexNet_experiment_0111          12         0.0010       0.5   0.01   \n",
       "8   runs/AlexNet_experiment_0200          12         0.0050       0.9   0.10   \n",
       "9   runs/AlexNet_experiment_0201          12         0.0050       0.9   0.01   \n",
       "10  runs/AlexNet_experiment_0210          12         0.0050       0.5   0.10   \n",
       "11  runs/AlexNet_experiment_0211          12         0.0050       0.5   0.01   \n",
       "\n",
       "   optimizer    time(m)  train_acc  val_acc  test_acc  \n",
       "0        sgd  11.339211    0.16550    0.164     0.166  \n",
       "1        sgd  10.766934    0.16000    0.154     0.154  \n",
       "2        sgd  10.802704    0.10650    0.110     0.112  \n",
       "3        sgd  10.881917    0.10325    0.098     0.098  \n",
       "4        sgd  10.824979    0.77850    0.538     0.546  \n",
       "5        sgd  10.823393    0.72800    0.518     0.534  \n",
       "6        sgd  11.056019    0.31725    0.314     0.276  \n",
       "7        sgd  11.018509    0.32650    0.316     0.304  \n",
       "8        sgd  11.177425    0.68500    0.526     0.534  \n",
       "9        sgd  10.675909    0.70275    0.508     0.518  \n",
       "10       sgd  10.631684    0.72800    0.520     0.502  \n",
       "11       sgd  10.594885    0.73750    0.552     0.534  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gridsearch across parameters to pick out best hyperparameters for this subset of data. Using previous functions to produce results.\n",
    "Saves information, metrics, and parameters in DataFrame.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "acc = []\n",
    "inputs = []\n",
    "\n",
    "trials = len(params['batch_size']) * len(params['lr']) * len(params['momentum']) * len(params['gamma'])\n",
    "trial = 0\n",
    "\n",
    "epoch = params['epoch'][0]\n",
    "\n",
    "for i, batch in enumerate(params['batch_size']):\n",
    "    for j, lr in enumerate(params['learning_rate']):\n",
    "        for k, mom in enumerate(params['momentum']):\n",
    "            for l, gamma in enumerate(params['gamma']):\n",
    "                for m, opt in enumerate(params['optimizer']):\n",
    "\n",
    "                    #print('', end='\\r')\n",
    "                    clear_output(wait=True)\n",
    "                    print('Trial {}/{}'.format(trial+1, trials))\n",
    "\n",
    "                    trial += 1\n",
    "                    data,sizes,classes = load_data('tinyimage10', batch_size = batch)\n",
    "                    labels_list, labels_dict = load_labels(\"tinyimage10\\\\words.txt\")\n",
    "\n",
    "                    model = AlexNet()\n",
    "                    file = 'runs/AlexNet_experiment_' + str(i) + str(j) + str(k)  + str(l)\n",
    "                    \n",
    "                    writer = SummaryWriter(file)\n",
    "                    print(file)\n",
    "\n",
    "                    if(opt == 'sgd'):\n",
    "                    # Cross entropy loss takes the logits directly, so we don't need to apply softmax in our CNN\n",
    "                        loss_func = nn.CrossEntropyLoss()\n",
    "                        optimizer = optim.SGD(model.parameters(), lr=lr, momentum = mom)\n",
    "                        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30,gamma=gamma)\n",
    "\n",
    "                    model = model.to(device)\n",
    "\n",
    "                    \n",
    "                    m, t = train(model, data, loss_func, scheduler, optimizer, num_epochs=epoch, lr=lr, momentum = mom, step_size=30, gamma=gamma)\n",
    "                    inputs.append([file, batch, lr, mom, gamma,opt,t])\n",
    "                    \n",
    "                    train_acc = calc_acc(data['train'], labels_acc = False)\n",
    "                    val_acc = calc_acc(data['val'], labels_acc = False)\n",
    "                    test_acc = calc_acc(data['test'], labels_acc = False)\n",
    "                    acc.append([train_acc, val_acc, test_acc])\n",
    "\n",
    "\n",
    "\n",
    "results = pd.concat([pd.DataFrame(inputs), pd.DataFrame(acc)], axis=1)\n",
    "results.columns = ['file','batch_size', \"learning_rate\", \"momentum\", 'gamma','optimizer','time(m)','train_acc','val_acc','test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>momentum</th>\n",
       "      <th>gamma</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>time(m)</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>runs/AlexNet_experiment_0100</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.824979</td>\n",
       "      <td>0.77850</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>runs/AlexNet_experiment_0101</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.823393</td>\n",
       "      <td>0.72800</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>runs/AlexNet_experiment_0200</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>11.177425</td>\n",
       "      <td>0.68500</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>runs/AlexNet_experiment_0211</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.594885</td>\n",
       "      <td>0.73750</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>runs/AlexNet_experiment_0201</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.675909</td>\n",
       "      <td>0.70275</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>runs/AlexNet_experiment_0210</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.631684</td>\n",
       "      <td>0.72800</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>runs/AlexNet_experiment_0111</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>11.018509</td>\n",
       "      <td>0.32650</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>runs/AlexNet_experiment_0110</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>11.056019</td>\n",
       "      <td>0.31725</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>runs/AlexNet_experiment_0000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>11.339211</td>\n",
       "      <td>0.16550</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>runs/AlexNet_experiment_0001</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.766934</td>\n",
       "      <td>0.16000</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>runs/AlexNet_experiment_0010</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.802704</td>\n",
       "      <td>0.10650</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>runs/AlexNet_experiment_0011</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.881917</td>\n",
       "      <td>0.10325</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file  batch_size  learning_rate  momentum  gamma  \\\n",
       "0   runs/AlexNet_experiment_0100          12         0.0010       0.9   0.10   \n",
       "1   runs/AlexNet_experiment_0101          12         0.0010       0.9   0.01   \n",
       "2   runs/AlexNet_experiment_0200          12         0.0050       0.9   0.10   \n",
       "3   runs/AlexNet_experiment_0211          12         0.0050       0.5   0.01   \n",
       "4   runs/AlexNet_experiment_0201          12         0.0050       0.9   0.01   \n",
       "5   runs/AlexNet_experiment_0210          12         0.0050       0.5   0.10   \n",
       "6   runs/AlexNet_experiment_0111          12         0.0010       0.5   0.01   \n",
       "7   runs/AlexNet_experiment_0110          12         0.0010       0.5   0.10   \n",
       "8   runs/AlexNet_experiment_0000          12         0.0001       0.9   0.10   \n",
       "9   runs/AlexNet_experiment_0001          12         0.0001       0.9   0.01   \n",
       "10  runs/AlexNet_experiment_0010          12         0.0001       0.5   0.10   \n",
       "11  runs/AlexNet_experiment_0011          12         0.0001       0.5   0.01   \n",
       "\n",
       "   optimizer    time(m)  train_acc  val_acc  test_acc  \n",
       "0        sgd  10.824979    0.77850    0.538     0.546  \n",
       "1        sgd  10.823393    0.72800    0.518     0.534  \n",
       "2        sgd  11.177425    0.68500    0.526     0.534  \n",
       "3        sgd  10.594885    0.73750    0.552     0.534  \n",
       "4        sgd  10.675909    0.70275    0.508     0.518  \n",
       "5        sgd  10.631684    0.72800    0.520     0.502  \n",
       "6        sgd  11.018509    0.32650    0.316     0.304  \n",
       "7        sgd  11.056019    0.31725    0.314     0.276  \n",
       "8        sgd  11.339211    0.16550    0.164     0.166  \n",
       "9        sgd  10.766934    0.16000    0.154     0.154  \n",
       "10       sgd  10.802704    0.10650    0.110     0.112  \n",
       "11       sgd  10.881917    0.10325    0.098     0.098  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = results.sort_values(by='test_acc', ascending=False).reset_index(drop=True)\n",
    "best = results.iloc[0,]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file             runs/AlexNet_experiment_0100\n",
       "batch_size                                 12\n",
       "learning_rate                           0.001\n",
       "momentum                                  0.9\n",
       "gamma                                     0.1\n",
       "optimizer                                 sgd\n",
       "time(m)                                10.825\n",
       "train_acc                              0.7785\n",
       "val_acc                                 0.538\n",
       "test_acc                                0.546\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': [12],\n",
       " 'learning_rate': [0.0001, 0.001, 0.005],\n",
       " 'momentum': [0.9, 0.5],\n",
       " 'gamma': [0.1, 0.01],\n",
       " 'optimizer': ['sgd'],\n",
       " 'epoch': [35]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 12,\n",
       " 'learning_rate': 0.001,\n",
       " 'momentum': 0.9,\n",
       " 'gamma': 0.1,\n",
       " 'optimizer': 'sgd',\n",
       " 'epoch': 35}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = params\n",
    "for i ,(key, value) in enumerate(params.items()):\n",
    "    if(key in best):\n",
    "        best_params[key] = best[key]\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 12, learning_rate: 0.001, momentum: 0.9, gamma: 0.1, epochs: 35\n",
      "Datasets Loaded\n",
      "Labels Loaded\n",
      "runs/AlexNet_experiment_best\n",
      "Epoch 1/35\n",
      "---------------\n",
      "Epoch 1: 0/334 step in progress\n",
      "Epoch 1: 200/334 step in progress\n",
      "train Loss: 3.1651 Acc: 0.0995\n",
      "Epoch 1: 0/334 step in progress\n",
      "val Loss: 2.4213 Acc: 0.1000\n",
      "\n",
      "Epoch 2/35\n",
      "---------------\n",
      "Epoch 2: 0/334 step in progress\n",
      "Epoch 2: 200/334 step in progress\n",
      "train Loss: 2.3918 Acc: 0.0975\n",
      "Epoch 2: 0/334 step in progress\n",
      "val Loss: 2.3817 Acc: 0.1000\n",
      "\n",
      "Epoch 3/35\n",
      "---------------\n",
      "Epoch 3: 0/334 step in progress\n",
      "Epoch 3: 200/334 step in progress\n",
      "train Loss: 2.3578 Acc: 0.1008\n",
      "Epoch 3: 0/334 step in progress\n",
      "val Loss: 2.3494 Acc: 0.1360\n",
      "\n",
      "Epoch 4/35\n",
      "---------------\n",
      "Epoch 4: 0/334 step in progress\n",
      "Epoch 4: 200/334 step in progress\n",
      "train Loss: 2.3471 Acc: 0.0975\n",
      "Epoch 4: 0/334 step in progress\n",
      "val Loss: 2.3121 Acc: 0.1100\n",
      "\n",
      "Epoch 5/35\n",
      "---------------\n",
      "Epoch 5: 0/334 step in progress\n",
      "Epoch 5: 200/334 step in progress\n",
      "train Loss: 2.3018 Acc: 0.1260\n",
      "Epoch 5: 0/334 step in progress\n",
      "val Loss: 2.2589 Acc: 0.1460\n",
      "\n",
      "Epoch 6/35\n",
      "---------------\n",
      "Epoch 6: 0/334 step in progress\n",
      "Epoch 6: 200/334 step in progress\n",
      "train Loss: 2.2212 Acc: 0.1573\n",
      "Epoch 6: 0/334 step in progress\n",
      "val Loss: 2.2893 Acc: 0.1280\n",
      "\n",
      "Epoch 7/35\n",
      "---------------\n",
      "Epoch 7: 0/334 step in progress\n",
      "Epoch 7: 200/334 step in progress\n",
      "train Loss: 2.1247 Acc: 0.1920\n",
      "Epoch 7: 0/334 step in progress\n",
      "val Loss: 2.1534 Acc: 0.1880\n",
      "\n",
      "Epoch 8/35\n",
      "---------------\n",
      "Epoch 8: 0/334 step in progress\n",
      "Epoch 8: 200/334 step in progress\n",
      "train Loss: 2.0288 Acc: 0.2430\n",
      "Epoch 8: 0/334 step in progress\n",
      "val Loss: 2.0160 Acc: 0.2420\n",
      "\n",
      "Epoch 9/35\n",
      "---------------\n",
      "Epoch 9: 0/334 step in progress\n",
      "Epoch 9: 200/334 step in progress\n",
      "train Loss: 1.9454 Acc: 0.2792\n",
      "Epoch 9: 0/334 step in progress\n",
      "val Loss: 2.0273 Acc: 0.2620\n",
      "\n",
      "Epoch 10/35\n",
      "---------------\n",
      "Epoch 10: 0/334 step in progress\n",
      "Epoch 10: 200/334 step in progress\n",
      "train Loss: 1.8685 Acc: 0.3050\n",
      "Epoch 10: 0/334 step in progress\n",
      "val Loss: 1.8491 Acc: 0.3160\n",
      "\n",
      "Epoch 11/35\n",
      "---------------\n",
      "Epoch 11: 0/334 step in progress\n",
      "Epoch 11: 200/334 step in progress\n",
      "train Loss: 1.8248 Acc: 0.3193\n",
      "Epoch 11: 0/334 step in progress\n",
      "val Loss: 2.0332 Acc: 0.3000\n",
      "\n",
      "Epoch 12/35\n",
      "---------------\n",
      "Epoch 12: 0/334 step in progress\n",
      "Epoch 12: 200/334 step in progress\n",
      "train Loss: 1.7870 Acc: 0.3432\n",
      "Epoch 12: 0/334 step in progress\n",
      "val Loss: 1.8668 Acc: 0.3260\n",
      "\n",
      "Epoch 13/35\n",
      "---------------\n",
      "Epoch 13: 0/334 step in progress\n",
      "Epoch 13: 200/334 step in progress\n",
      "train Loss: 1.7346 Acc: 0.3608\n",
      "Epoch 13: 0/334 step in progress\n",
      "val Loss: 1.7881 Acc: 0.3560\n",
      "\n",
      "Epoch 14/35\n",
      "---------------\n",
      "Epoch 14: 0/334 step in progress\n",
      "Epoch 14: 200/334 step in progress\n",
      "train Loss: 1.6839 Acc: 0.3857\n",
      "Epoch 14: 0/334 step in progress\n",
      "val Loss: 1.7893 Acc: 0.3680\n",
      "\n",
      "Epoch 15/35\n",
      "---------------\n",
      "Epoch 15: 0/334 step in progress\n",
      "Epoch 15: 200/334 step in progress\n",
      "train Loss: 1.6276 Acc: 0.4098\n",
      "Epoch 15: 0/334 step in progress\n",
      "val Loss: 1.7526 Acc: 0.4000\n",
      "\n",
      "Epoch 16/35\n",
      "---------------\n",
      "Epoch 16: 0/334 step in progress\n",
      "Epoch 16: 200/334 step in progress\n",
      "train Loss: 1.5763 Acc: 0.4390\n",
      "Epoch 16: 0/334 step in progress\n",
      "val Loss: 1.7529 Acc: 0.4080\n",
      "\n",
      "Epoch 17/35\n",
      "---------------\n",
      "Epoch 17: 0/334 step in progress\n",
      "Epoch 17: 200/334 step in progress\n",
      "train Loss: 1.5262 Acc: 0.4582\n",
      "Epoch 17: 0/334 step in progress\n",
      "val Loss: 1.6373 Acc: 0.4780\n",
      "\n",
      "Epoch 18/35\n",
      "---------------\n",
      "Epoch 18: 0/334 step in progress\n",
      "Epoch 18: 200/334 step in progress\n",
      "train Loss: 1.4711 Acc: 0.4815\n",
      "Epoch 18: 0/334 step in progress\n",
      "val Loss: 1.5879 Acc: 0.4580\n",
      "\n",
      "Epoch 19/35\n",
      "---------------\n",
      "Epoch 19: 0/334 step in progress\n",
      "Epoch 19: 200/334 step in progress\n",
      "train Loss: 1.4096 Acc: 0.5030\n",
      "Epoch 19: 0/334 step in progress\n",
      "val Loss: 1.6661 Acc: 0.4240\n",
      "\n",
      "Epoch 20/35\n",
      "---------------\n",
      "Epoch 20: 0/334 step in progress\n",
      "Epoch 20: 200/334 step in progress\n",
      "train Loss: 1.3936 Acc: 0.5065\n",
      "Epoch 20: 0/334 step in progress\n",
      "val Loss: 1.5892 Acc: 0.4540\n",
      "\n",
      "Epoch 21/35\n",
      "---------------\n",
      "Epoch 21: 0/334 step in progress\n",
      "Epoch 21: 200/334 step in progress\n",
      "train Loss: 1.3418 Acc: 0.5345\n",
      "Epoch 21: 0/334 step in progress\n",
      "val Loss: 1.6515 Acc: 0.4500\n",
      "\n",
      "Epoch 22/35\n",
      "---------------\n",
      "Epoch 22: 0/334 step in progress\n",
      "Epoch 22: 200/334 step in progress\n",
      "train Loss: 1.2950 Acc: 0.5460\n",
      "Epoch 22: 0/334 step in progress\n",
      "val Loss: 1.6996 Acc: 0.4620\n",
      "\n",
      "Epoch 23/35\n",
      "---------------\n",
      "Epoch 23: 0/334 step in progress\n",
      "Epoch 23: 200/334 step in progress\n",
      "train Loss: 1.2397 Acc: 0.5633\n",
      "Epoch 23: 0/334 step in progress\n",
      "val Loss: 1.5475 Acc: 0.5220\n",
      "\n",
      "Epoch 24/35\n",
      "---------------\n",
      "Epoch 24: 0/334 step in progress\n",
      "Epoch 24: 200/334 step in progress\n",
      "train Loss: 1.1996 Acc: 0.5683\n",
      "Epoch 24: 0/334 step in progress\n",
      "val Loss: 1.5748 Acc: 0.4940\n",
      "\n",
      "Epoch 25/35\n",
      "---------------\n",
      "Epoch 25: 0/334 step in progress\n",
      "Epoch 25: 200/334 step in progress\n",
      "train Loss: 1.1444 Acc: 0.5988\n",
      "Epoch 25: 0/334 step in progress\n",
      "val Loss: 1.5336 Acc: 0.5280\n",
      "\n",
      "Epoch 26/35\n",
      "---------------\n",
      "Epoch 26: 0/334 step in progress\n",
      "Epoch 26: 200/334 step in progress\n",
      "train Loss: 1.0989 Acc: 0.6192\n",
      "Epoch 26: 0/334 step in progress\n",
      "val Loss: 1.5109 Acc: 0.5180\n",
      "\n",
      "Epoch 27/35\n",
      "---------------\n",
      "Epoch 27: 0/334 step in progress\n",
      "Epoch 27: 200/334 step in progress\n",
      "train Loss: 1.0746 Acc: 0.6178\n",
      "Epoch 27: 0/334 step in progress\n",
      "val Loss: 1.5365 Acc: 0.5100\n",
      "\n",
      "Epoch 28/35\n",
      "---------------\n",
      "Epoch 28: 0/334 step in progress\n",
      "Epoch 28: 200/334 step in progress\n",
      "train Loss: 1.0546 Acc: 0.6298\n",
      "Epoch 28: 0/334 step in progress\n",
      "val Loss: 1.5177 Acc: 0.5240\n",
      "\n",
      "Epoch 29/35\n",
      "---------------\n",
      "Epoch 29: 0/334 step in progress\n",
      "Epoch 29: 200/334 step in progress\n",
      "train Loss: 0.9964 Acc: 0.6455\n",
      "Epoch 29: 0/334 step in progress\n",
      "val Loss: 1.6654 Acc: 0.4840\n",
      "\n",
      "Epoch 30/35\n",
      "---------------\n",
      "Epoch 30: 0/334 step in progress\n",
      "Epoch 30: 200/334 step in progress\n",
      "train Loss: 0.9624 Acc: 0.6640\n",
      "Epoch 30: 0/334 step in progress\n",
      "val Loss: 1.7054 Acc: 0.4880\n",
      "\n",
      "Epoch 31/35\n",
      "---------------\n",
      "Epoch 31: 0/334 step in progress\n",
      "Epoch 31: 200/334 step in progress\n",
      "train Loss: 0.9346 Acc: 0.6718\n",
      "Epoch 31: 0/334 step in progress\n",
      "val Loss: 1.6764 Acc: 0.5100\n",
      "\n",
      "Epoch 32/35\n",
      "---------------\n",
      "Epoch 32: 0/334 step in progress\n",
      "Epoch 32: 200/334 step in progress\n",
      "train Loss: 0.8664 Acc: 0.6883\n",
      "Epoch 32: 0/334 step in progress\n",
      "val Loss: 1.6516 Acc: 0.5220\n",
      "\n",
      "Epoch 33/35\n",
      "---------------\n",
      "Epoch 33: 0/334 step in progress\n",
      "Epoch 33: 200/334 step in progress\n",
      "train Loss: 0.8605 Acc: 0.6897\n",
      "Epoch 33: 0/334 step in progress\n",
      "val Loss: 1.7788 Acc: 0.4840\n",
      "\n",
      "Epoch 34/35\n",
      "---------------\n",
      "Epoch 34: 0/334 step in progress\n",
      "Epoch 34: 200/334 step in progress\n",
      "train Loss: 0.8360 Acc: 0.7100\n",
      "Epoch 34: 0/334 step in progress\n",
      "val Loss: 1.5754 Acc: 0.5240\n",
      "\n",
      "Epoch 35/35\n",
      "---------------\n",
      "Epoch 35: 0/334 step in progress\n",
      "Epoch 35: 200/334 step in progress\n",
      "train Loss: 0.7981 Acc: 0.7195\n",
      "Epoch 35: 0/334 step in progress\n",
      "val Loss: 1.6851 Acc: 0.5340\n",
      "\n",
      "Training complete in 11m 36s\n"
     ]
    }
   ],
   "source": [
    "batch = int(best_params['batch_size'])\n",
    "lr = float(best_params['learning_rate'])\n",
    "mom = float(best_params['momentum'])\n",
    "gamma = float(best_params['gamma'])\n",
    "epoch = best_params['epoch']\n",
    "\n",
    "print('batch_size: {}, learning_rate: {}, momentum: {}, gamma: {}, epochs: {}'.format(batch,lr,mom,gamma,epoch))\n",
    "\n",
    "data,sizes,classes = load_data('tinyimage10', batch_size = batch)\n",
    "labels_list, labels_dict = load_labels(\"tinyimage10\\\\words.txt\")\n",
    "\n",
    "model = AlexNet()\n",
    "file = 'runs/AlexNet_experiment_best'\n",
    "\n",
    "writer = SummaryWriter(file)\n",
    "print(file)\n",
    "\n",
    "\n",
    "# Cross entropy loss takes the logits directly, so we don't need to apply softmax in our CNN\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum = mom)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30,gamma=gamma)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "m, t = train(model, data, loss_func, scheduler, optimizer, num_epochs=epoch, lr=lr, momentum = mom, step_size=30, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('results.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>momentum</th>\n",
       "      <th>gamma</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>time(m)</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>runs/AlexNet_experiment_0100</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.824979</td>\n",
       "      <td>0.77850</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>runs/AlexNet_experiment_0101</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.823393</td>\n",
       "      <td>0.72800</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>runs/AlexNet_experiment_0200</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>11.177425</td>\n",
       "      <td>0.68500</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>runs/AlexNet_experiment_0211</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.594885</td>\n",
       "      <td>0.73750</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>runs/AlexNet_experiment_0201</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.675909</td>\n",
       "      <td>0.70275</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>runs/AlexNet_experiment_0210</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.631684</td>\n",
       "      <td>0.72800</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>runs/AlexNet_experiment_0111</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>11.018509</td>\n",
       "      <td>0.32650</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>runs/AlexNet_experiment_0110</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>11.056019</td>\n",
       "      <td>0.31725</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>runs/AlexNet_experiment_0000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>11.339211</td>\n",
       "      <td>0.16550</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>runs/AlexNet_experiment_0001</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.766934</td>\n",
       "      <td>0.16000</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>runs/AlexNet_experiment_0010</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.802704</td>\n",
       "      <td>0.10650</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>runs/AlexNet_experiment_0011</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10.881917</td>\n",
       "      <td>0.10325</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file  batch_size  learning_rate  momentum  gamma  \\\n",
       "0   runs/AlexNet_experiment_0100          12         0.0010       0.9   0.10   \n",
       "1   runs/AlexNet_experiment_0101          12         0.0010       0.9   0.01   \n",
       "2   runs/AlexNet_experiment_0200          12         0.0050       0.9   0.10   \n",
       "3   runs/AlexNet_experiment_0211          12         0.0050       0.5   0.01   \n",
       "4   runs/AlexNet_experiment_0201          12         0.0050       0.9   0.01   \n",
       "5   runs/AlexNet_experiment_0210          12         0.0050       0.5   0.10   \n",
       "6   runs/AlexNet_experiment_0111          12         0.0010       0.5   0.01   \n",
       "7   runs/AlexNet_experiment_0110          12         0.0010       0.5   0.10   \n",
       "8   runs/AlexNet_experiment_0000          12         0.0001       0.9   0.10   \n",
       "9   runs/AlexNet_experiment_0001          12         0.0001       0.9   0.01   \n",
       "10  runs/AlexNet_experiment_0010          12         0.0001       0.5   0.10   \n",
       "11  runs/AlexNet_experiment_0011          12         0.0001       0.5   0.01   \n",
       "\n",
       "   optimizer    time(m)  train_acc  val_acc  test_acc  \n",
       "0        sgd  10.824979    0.77850    0.538     0.546  \n",
       "1        sgd  10.823393    0.72800    0.518     0.534  \n",
       "2        sgd  11.177425    0.68500    0.526     0.534  \n",
       "3        sgd  10.594885    0.73750    0.552     0.534  \n",
       "4        sgd  10.675909    0.70275    0.508     0.518  \n",
       "5        sgd  10.631684    0.72800    0.520     0.502  \n",
       "6        sgd  11.018509    0.32650    0.316     0.304  \n",
       "7        sgd  11.056019    0.31725    0.314     0.276  \n",
       "8        sgd  11.339211    0.16550    0.164     0.166  \n",
       "9        sgd  10.766934    0.16000    0.154     0.154  \n",
       "10       sgd  10.802704    0.10650    0.110     0.112  \n",
       "11       sgd  10.881917    0.10325    0.098     0.098  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.read_csv('results.csv')\n",
    "temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
